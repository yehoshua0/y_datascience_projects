{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a873cd10",
   "metadata": {
    "papermill": {
     "duration": 0.006643,
     "end_time": "2025-10-05T11:40:31.906380",
     "exception": false,
     "start_time": "2025-10-05T11:40:31.899737",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ANIP Challenge | Tâche 1 : Collecte & Préparation des Données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5814be",
   "metadata": {
    "papermill": {
     "duration": 0.005013,
     "end_time": "2025-10-05T11:40:31.917457",
     "exception": false,
     "start_time": "2025-10-05T11:40:31.912444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "L’Agence Nationale d’Identification des Personnes (ANIP) est au cœur de la gestion des données d’état civil au Bénin. Mais pour mieux comprendre les dynamiques de développement, il est essentiel de croiser ces données avec des sources démographiques, économiques, sociales et sportives.\n",
    "Ce challenge invite les participants à collecter, analyser et valoriser ces données à travers des dashboards interactifs et percutants dans Power BI.\n",
    "\n",
    "L’objectif : révéler des tendances cachées, détecter des anomalies et produire des insights exploitables pour orienter les politiques publiques et renforcer l’impact des événements sportifs et sociaux.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5625dfc4",
   "metadata": {
    "papermill": {
     "duration": 0.00513,
     "end_time": "2025-10-05T11:40:31.928734",
     "exception": false,
     "start_time": "2025-10-05T11:40:31.923604",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Description de la tâche\n",
    "\n",
    "Objectif\n",
    "\n",
    "Cette tâche vise à tester la capacité des candidats à rechercher, collecter et consolider des données provenant de sources variées (bases internationales, portails open data, scraping de sites web, rapports publics,). Ils devront montrer leur maîtrise pour :\n",
    "\n",
    "* identifier des sources diverses (internationales, nationales, portails open data, API, rapports, fédérations sportives, etc.),\n",
    "* préparer un notebook pour la tâche 1\n",
    "    * collecter des jeux de données (téléchargement, scraping, API),\n",
    "    * nettoyer & harmoniser les données (unités, périodes, découpages géographiques, formats),\n",
    "    * consolider plusieurs sources variées dans un dataset final prêt à l’analyse.\n",
    "* documenter et consolider dans un dataset unique prêt à l’analyse.\n",
    "\n",
    "Si besoins vous pouvez mettre vos données à disposition sur un dépôt GIT, un drive ou autres.\n",
    "\n",
    "\n",
    "**Exemple de sources et Jeux de données**\n",
    "\n",
    "Démographiques\n",
    "\n",
    "* https://hub.worldpop.org/project/categories?id=3\n",
    "* https://population.un.org/wpp/downloads?folder=Standard%20Projections&group=Most%20used\n",
    "* https://dhsprogram.com/data/available-datasets.cfm\n",
    "\n",
    "Économiques\n",
    "\n",
    "* https://data.imf.org/en/Datasets#t=coveo117bcfc4&sort=%40idata_publication_date%20descending\n",
    "* https://www.oecd.org/en/data.html\n",
    "* https://unctadstat.unctad.org/EN/\n",
    "\n",
    "Sociales\n",
    "\n",
    "* https://www.who.int/data/gho\n",
    "* https://hdr.undp.org/data-center\n",
    "* https://genderdata.worldbank.org/en/home\n",
    "\n",
    "Livrables attendus\n",
    "\n",
    "* Un ou plusieurs datasets finaux (CSV / Excel) proprement nettoyés, harmonisés, documentés, prêt à l’analyse et mis à disposition sur un GIT ou Drive\n",
    "* Un script ou notebook (Python) ou plusieurs modules : collecte (scraping / API), nettoyage, harmonisation.\n",
    "* Un glossaire ou dictionnaire des variables utilisées : noms, définitions, unité, source, période, géographie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94455d50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T11:40:31.942607Z",
     "iopub.status.busy": "2025-10-05T11:40:31.942175Z",
     "iopub.status.idle": "2025-10-05T11:41:38.655705Z",
     "shell.execute_reply": "2025-10-05T11:41:38.654415Z"
    },
    "papermill": {
     "duration": 66.721976,
     "end_time": "2025-10-05T11:41:38.657434",
     "exception": false,
     "start_time": "2025-10-05T11:40:31.935458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation des dépendances...\n",
      "Installation de pandas>=2.0.0...\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0) (2025.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=2.0.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=2.0.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=2.0.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=2.0.0) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=2.0.0) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas>=2.0.0) (2.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas>=2.0.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas>=2.0.0) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas>=2.0.0) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas>=2.0.0) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas>=2.0.0) (2024.2.0)\n",
      "Installation de numpy>=1.24.0...\n",
      "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.0) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.0) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.24.0) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.24.0) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.24.0) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.24.0) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.24.0) (2024.2.0)\n",
      "Installation de requests>=2.31.0...\n",
      "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0) (2025.6.15)\n",
      "Installation de beautifulsoup4>=4.12.0...\n",
      "Requirement already satisfied: beautifulsoup4>=4.12.0 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.12.0) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.12.0) (4.14.0)\n",
      "Installation de openpyxl>=3.1.0...\n",
      "Requirement already satisfied: openpyxl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl>=3.1.0) (2.0.0)\n",
      "Installation de xlrd>=2.0.1...\n",
      "Requirement already satisfied: xlrd>=2.0.1 in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
      "Installation de lxml>=4.9.0...\n",
      "Requirement already satisfied: lxml>=4.9.0 in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
      "Installation de selenium>=4.15.0...\n",
      "Collecting selenium>=4.15.0\n",
      "  Downloading selenium-4.36.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium>=4.15.0) (2.5.0)\n",
      "Collecting trio<1.0,>=0.30.0 (from selenium>=4.15.0)\n",
      "  Downloading trio-0.31.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket<1.0,>=0.12.2 (from selenium>=4.15.0)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.11/dist-packages (from selenium>=4.15.0) (2025.6.15)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.14.0 in /usr/local/lib/python3.11/dist-packages (from selenium>=4.15.0) (4.14.0)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium>=4.15.0) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio<1.0,>=0.30.0->selenium>=4.15.0) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio<1.0,>=0.30.0->selenium>=4.15.0) (2.4.0)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio<1.0,>=0.30.0->selenium>=4.15.0) (3.10)\n",
      "Collecting outcome (from trio<1.0,>=0.30.0->selenium>=4.15.0)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio<1.0,>=0.30.0->selenium>=4.15.0) (1.3.1)\n",
      "Collecting wsproto>=0.14 (from trio-websocket<1.0,>=0.12.2->selenium>=4.15.0)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium>=4.15.0) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium>=4.15.0) (0.16.0)\n",
      "Downloading selenium-4.36.0-py3-none-any.whl (9.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.6/9.6 MB 64.2 MB/s eta 0:00:00\n",
      "Downloading trio-0.31.0-py3-none-any.whl (512 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 512.7/512.7 kB 19.5 MB/s eta 0:00:00\n",
      "Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: wsproto, outcome, trio, trio-websocket, selenium\n",
      "Successfully installed outcome-1.3.0.post0 selenium-4.36.0 trio-0.31.0 trio-websocket-0.12.2 wsproto-1.2.0\n",
      "Installation de pycountry>=22.3.0...\n",
      "Collecting pycountry>=22.3.0\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.3/6.3 MB 44.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pycountry\n",
      "Successfully installed pycountry-24.6.1\n",
      "Installation de python-dotenv>=1.0.0...\n",
      "Collecting python-dotenv>=1.0.0\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.1\n",
      "Installation de tqdm>=4.66.0...\n",
      "Requirement already satisfied: tqdm>=4.66.0 in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Installation de matplotlib>=3.7.0...\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (11.2.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0) (2.9.0.post0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib>=3.7.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib>=3.7.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib>=3.7.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib>=3.7.0) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib>=3.7.0) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib>=3.7.0) (2.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->matplotlib>=3.7.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->matplotlib>=3.7.0) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.20->matplotlib>=3.7.0) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.20->matplotlib>=3.7.0) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.20->matplotlib>=3.7.0) (2024.2.0)\n",
      "Installation de seaborn>=0.12.0...\n",
      "Requirement already satisfied: seaborn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (0.12.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/lib/python3.11/dist-packages (from seaborn>=0.12.0) (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.12.0) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.12.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.12.0) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.12.0) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.12.0) (25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.12.0) (11.2.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.12.0) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn>=0.12.0) (2.9.0.post0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.17->seaborn>=0.12.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.17->seaborn>=0.12.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.17->seaborn>=0.12.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.17->seaborn>=0.12.0) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.17->seaborn>=0.12.0) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy!=1.24.0,>=1.17->seaborn>=0.12.0) (2.4.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25->seaborn>=0.12.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25->seaborn>=0.12.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn>=0.12.0) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy!=1.24.0,>=1.17->seaborn>=0.12.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy!=1.24.0,>=1.17->seaborn>=0.12.0) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy!=1.24.0,>=1.17->seaborn>=0.12.0) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy!=1.24.0,>=1.17->seaborn>=0.12.0) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy!=1.24.0,>=1.17->seaborn>=0.12.0) (2024.2.0)\n",
      "Installation de plotly>=5.17.0...\n",
      "Requirement already satisfied: plotly>=5.17.0 in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.17.0) (8.5.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly>=5.17.0) (25.0)\n",
      "✓ Toutes les dépendances sont installées!\n",
      "\n",
      "Vérification des installations...\n",
      "✓ pandas - OK\n",
      "✓ numpy - OK\n",
      "✓ requests - OK\n",
      "✓ bs4 - OK\n",
      "✓ openpyxl - OK\n",
      "✓ selenium - OK\n",
      "✓ pycountry - OK\n",
      "✓ tqdm - OK\n",
      "\n",
      "✓ Tous les packages sont correctement installés!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ANIP Challenge - Configuration et Installation des Dépendances\n",
    "==============================================================\n",
    "Script d'installation des bibliothèques nécessaires pour le projet\n",
    "\"\"\"\n",
    "\n",
    "# Liste des packages requis\n",
    "REQUIRED_PACKAGES = [\n",
    "    'pandas>=2.0.0',\n",
    "    'numpy>=1.24.0',\n",
    "    'requests>=2.31.0',\n",
    "    'beautifulsoup4>=4.12.0',\n",
    "    'openpyxl>=3.1.0',\n",
    "    'xlrd>=2.0.1',\n",
    "    'lxml>=4.9.0',\n",
    "    'selenium>=4.15.0',\n",
    "    'pycountry>=22.3.0',\n",
    "    'python-dotenv>=1.0.0',\n",
    "    'tqdm>=4.66.0',\n",
    "    'matplotlib>=3.7.0',\n",
    "    'seaborn>=0.12.0',\n",
    "    'plotly>=5.17.0'\n",
    "]\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Installe tous les packages nécessaires\"\"\"\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    print(\"Installation des dépendances...\")\n",
    "    for package in REQUIRED_PACKAGES:\n",
    "        print(f\"Installation de {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "    print(\"✓ Toutes les dépendances sont installées!\")\n",
    "\n",
    "def verify_installation():\n",
    "    \"\"\"Vérifie que tous les packages sont correctement installés\"\"\"\n",
    "    import importlib\n",
    "    \n",
    "    packages_to_check = [\n",
    "        'pandas', 'numpy', 'requests', 'bs4', \n",
    "        'openpyxl', 'selenium', 'pycountry', 'tqdm'\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nVérification des installations...\")\n",
    "    all_ok = True\n",
    "    for package in packages_to_check:\n",
    "        try:\n",
    "            importlib.import_module(package)\n",
    "            print(f\"✓ {package} - OK\")\n",
    "        except ImportError:\n",
    "            print(f\"✗ {package} - ERREUR\")\n",
    "            all_ok = False\n",
    "    \n",
    "    if all_ok:\n",
    "        print(\"\\n✓ Tous les packages sont correctement installés!\")\n",
    "    else:\n",
    "        print(\"\\n✗ Certains packages n'ont pas pu être installés.\")\n",
    "    \n",
    "    return all_ok\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    install_packages()\n",
    "    verify_installation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0cbf458",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T11:41:38.675362Z",
     "iopub.status.busy": "2025-10-05T11:41:38.674775Z",
     "iopub.status.idle": "2025-10-05T11:41:38.688366Z",
     "shell.execute_reply": "2025-10-05T11:41:38.687220Z"
    },
    "papermill": {
     "duration": 0.024605,
     "end_time": "2025-10-05T11:41:38.690075",
     "exception": false,
     "start_time": "2025-10-05T11:41:38.665470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration chargée pour Benin\n",
      "✓ Répertoires créés : /kaggle/working/data\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Configuration Globale du Projet ANIP\n",
    "=====================================\n",
    "Contient toutes les configurations, chemins et paramètres du projet\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration des chemins\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "RAW_DATA_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
    "FINAL_DATA_DIR = DATA_DIR / \"final\"\n",
    "LOGS_DIR = BASE_DIR / \"logs\"\n",
    "DOCS_DIR = BASE_DIR / \"documentation\"\n",
    "\n",
    "# Création des répertoires\n",
    "for directory in [RAW_DATA_DIR, PROCESSED_DATA_DIR, FINAL_DATA_DIR, LOGS_DIR, DOCS_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Pays cible\n",
    "TARGET_COUNTRY = \"Benin\"\n",
    "TARGET_COUNTRY_CODE = \"BEN\"\n",
    "TARGET_COUNTRY_ISO2 = \"BJ\"\n",
    "\n",
    "# Période d'analyse\n",
    "START_YEAR = 2000\n",
    "END_YEAR = 2024\n",
    "CURRENT_YEAR = datetime.now().year\n",
    "\n",
    "# URLs des sources de données\n",
    "DATA_SOURCES = {\n",
    "    \"demographic\": {\n",
    "        \"worldpop\": \"https://hub.worldpop.org/geodata/listing?id=29\",\n",
    "        \"un_population\": \"https://population.un.org/wpp/Download/Standard/CSV/\",\n",
    "        \"dhs\": \"https://dhsprogram.com/data/available-datasets.cfm\"\n",
    "    },\n",
    "    \"economic\": {\n",
    "        \"imf\": \"https://www.imf.org/external/datamapper/api/v1\",\n",
    "        \"world_bank\": \"https://api.worldbank.org/v2/country/BEN/indicator\",\n",
    "        \"unctad\": \"https://unctadstat.unctad.org/datacentre/dataviewer/US.TotalMerchExports\"\n",
    "    },\n",
    "    \"social\": {\n",
    "        \"who\": \"https://ghoapi.azureedge.net/api/\",\n",
    "        \"undp\": \"https://hdr.undp.org/sites/default/files/\",\n",
    "        \"world_bank_gender\": \"https://genderdata.worldbank.org/\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Indicateurs clés à collecter\n",
    "INDICATORS = {\n",
    "    \"demographic\": [\n",
    "        \"population_totale\",\n",
    "        \"population_urbaine\",\n",
    "        \"population_rurale\",\n",
    "        \"densite_population\",\n",
    "        \"taux_croissance_population\",\n",
    "        \"esperance_vie\",\n",
    "        \"taux_natalite\",\n",
    "        \"taux_mortalite\",\n",
    "        \"taux_fecondite\",\n",
    "        \"migration_nette\",\n",
    "        \"ratio_dependance\",\n",
    "        \"age_median\"\n",
    "    ],\n",
    "    \"economic\": [\n",
    "        \"pib_total\",\n",
    "        \"pib_par_habitant\",\n",
    "        \"taux_croissance_pib\",\n",
    "        \"inflation\",\n",
    "        \"taux_chomage\",\n",
    "        \"dette_publique\",\n",
    "        \"investissement_direct_etranger\",\n",
    "        \"exportations\",\n",
    "        \"importations\",\n",
    "        \"balance_commerciale\",\n",
    "        \"indice_pauvrete\",\n",
    "        \"coefficient_gini\"\n",
    "    ],\n",
    "    \"social\": [\n",
    "        \"taux_alphabetisation\",\n",
    "        \"taux_scolarisation_primaire\",\n",
    "        \"taux_scolarisation_secondaire\",\n",
    "        \"taux_acces_eau_potable\",\n",
    "        \"taux_acces_electricite\",\n",
    "        \"mortalite_infantile\",\n",
    "        \"mortalite_maternelle\",\n",
    "        \"prevalence_vih\",\n",
    "        \"acces_sante\",\n",
    "        \"indice_developpement_humain\",\n",
    "        \"inegalite_genre\",\n",
    "        \"violence_basee_genre\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Configuration du scraping\n",
    "SCRAPING_CONFIG = {\n",
    "    \"timeout\": 30,\n",
    "    \"retry_attempts\": 3,\n",
    "    \"delay_between_requests\": 2,\n",
    "    \"user_agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
    "}\n",
    "\n",
    "# Configuration des formats de données\n",
    "DATE_FORMAT = \"%Y-%m-%d\"\n",
    "DECIMAL_SEPARATOR = \".\"\n",
    "THOUSANDS_SEPARATOR = \",\"\n",
    "ENCODING = \"utf-8\"\n",
    "\n",
    "# Colonnes standardisées pour le dataset final\n",
    "STANDARD_COLUMNS = {\n",
    "    \"pays\": \"country\",\n",
    "    \"code_pays\": \"country_code\",\n",
    "    \"annee\": \"year\",\n",
    "    \"indicateur\": \"indicator\",\n",
    "    \"valeur\": \"value\",\n",
    "    \"unite\": \"unit\",\n",
    "    \"source\": \"source\",\n",
    "    \"date_collecte\": \"collection_date\",\n",
    "    \"fiabilite\": \"reliability_score\"\n",
    "}\n",
    "\n",
    "# Configuration des logs\n",
    "LOG_CONFIG = {\n",
    "    \"format\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    \"date_format\": \"%Y-%m-%d %H:%M:%S\",\n",
    "    \"level\": \"INFO\"\n",
    "}\n",
    "\n",
    "print(f\"✓ Configuration chargée pour {TARGET_COUNTRY}\")\n",
    "print(f\"✓ Répertoires créés : {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "706c96ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T11:41:38.707739Z",
     "iopub.status.busy": "2025-10-05T11:41:38.707397Z",
     "iopub.status.idle": "2025-10-05T11:42:03.301052Z",
     "shell.execute_reply": "2025-10-05T11:42:03.299902Z"
    },
    "papermill": {
     "duration": 24.604645,
     "end_time": "2025-10-05T11:42:03.302614",
     "exception": false,
     "start_time": "2025-10-05T11:41:38.697969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "World Bank: 100%|██████████| 14/14 [00:09<00:00,  1.40it/s]\n",
      "Collecte UN: 100%|██████████| 11/11 [00:14<00:00,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Collecte terminée: 2 fichiers créés dans data/raw/demographic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Module de Collecte des Données Démographiques\n",
    "==============================================\n",
    "Collecte des données depuis WorldPop, UN Population et DHS\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Configuration du logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class DemographicDataCollector:\n",
    "    \"\"\"Collecteur de données démographiques pour le Bénin\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir=\"data/raw/demographic\"):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.country_code = \"BEN\"\n",
    "        self.country_name = \"Benin\"\n",
    "        self.collected_data = []\n",
    "        \n",
    "    def collect_un_population_data(self):\n",
    "        \"\"\"\n",
    "        Collecte les données de population des Nations Unies\n",
    "        Source: UN World Population Prospects\n",
    "        \"\"\"\n",
    "        logger.info(\"Collecte des données UN Population...\")\n",
    "        \n",
    "        try:\n",
    "            # URL de l'API UN Data\n",
    "            base_url = \"https://population.un.org/dataportalapi/api/v1\"\n",
    "            \n",
    "            # Indicateurs démographiques principaux\n",
    "            indicators = {\n",
    "                49: \"Population totale\",\n",
    "                60: \"Population urbaine\",\n",
    "                61: \"Population rurale\",\n",
    "                68: \"Densité de population\",\n",
    "                53: \"Taux de croissance\",\n",
    "                58: \"Espérance de vie\",\n",
    "                19: \"Taux de natalité\",\n",
    "                20: \"Taux de mortalité\",\n",
    "                54: \"Taux de fécondité\",\n",
    "                24: \"Migration nette\",\n",
    "                48: \"Age médian\"\n",
    "            }\n",
    "            \n",
    "            all_data = []\n",
    "            \n",
    "            for indicator_id, indicator_name in tqdm(indicators.items(), desc=\"Collecte UN\"):\n",
    "                url = f\"{base_url}/data/indicators/{indicator_id}/locations/204/start/2000/end/2024\"\n",
    "                \n",
    "                try:\n",
    "                    response = requests.get(url, timeout=30)\n",
    "                    if response.status_code == 200:\n",
    "                        data = response.json()\n",
    "                        \n",
    "                        if 'data' in data:\n",
    "                            for record in data['data']:\n",
    "                                all_data.append({\n",
    "                                    'country': self.country_name,\n",
    "                                    'country_code': self.country_code,\n",
    "                                    'year': record.get('timeLabel', ''),\n",
    "                                    'indicator': indicator_name,\n",
    "                                    'value': record.get('value', np.nan),\n",
    "                                    'variant': record.get('variant', 'Medium'),\n",
    "                                    'sex': record.get('sex', 'Both'),\n",
    "                                    'age_group': record.get('ageLabel', 'All ages'),\n",
    "                                    'source': 'UN World Population Prospects',\n",
    "                                    'collection_date': datetime.now().strftime('%Y-%m-%d')\n",
    "                                })\n",
    "                    \n",
    "                    time.sleep(1)  # Respecter les limites de l'API\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Erreur pour l'indicateur {indicator_name}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            if all_data:\n",
    "                df = pd.DataFrame(all_data)\n",
    "                output_file = self.output_dir / \"un_population_data.csv\"\n",
    "                df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "                logger.info(f\"✓ {len(df)} enregistrements UN collectés\")\n",
    "                self.collected_data.append(df)\n",
    "                return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur collecte UN: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def collect_worldbank_population_data(self):\n",
    "        \"\"\"\n",
    "        Collecte les données démographiques de la Banque Mondiale\n",
    "        Alternative plus accessible que WorldPop\n",
    "        \"\"\"\n",
    "        logger.info(\"Collecte des données World Bank Population...\")\n",
    "        \n",
    "        try:\n",
    "            # Indicateurs de la Banque Mondiale\n",
    "            indicators = {\n",
    "                'SP.POP.TOTL': 'Population totale',\n",
    "                'SP.URB.TOTL': 'Population urbaine',\n",
    "                'SP.RUR.TOTL': 'Population rurale',\n",
    "                'SP.POP.GROW': 'Taux de croissance population',\n",
    "                'SP.DYN.LE00.IN': 'Espérance de vie',\n",
    "                'SP.DYN.CBRT.IN': 'Taux de natalité brut',\n",
    "                'SP.DYN.CDRT.IN': 'Taux de mortalité brut',\n",
    "                'SP.DYN.TFRT.IN': 'Taux de fécondité',\n",
    "                'EN.POP.DNST': 'Densité de population',\n",
    "                'SP.POP.DPND': 'Ratio de dépendance',\n",
    "                'SP.URB.TOTL.IN.ZS': 'Population urbaine %',\n",
    "                'SP.POP.0014.TO.ZS': 'Population 0-14 ans %',\n",
    "                'SP.POP.1564.TO.ZS': 'Population 15-64 ans %',\n",
    "                'SP.POP.65UP.TO.ZS': 'Population 65+ ans %'\n",
    "            }\n",
    "            \n",
    "            all_data = []\n",
    "            base_url = \"https://api.worldbank.org/v2/country/BEN/indicator\"\n",
    "            \n",
    "            for indicator_code, indicator_name in tqdm(indicators.items(), desc=\"World Bank\"):\n",
    "                url = f\"{base_url}/{indicator_code}?format=json&date=2000:2024&per_page=500\"\n",
    "                \n",
    "                try:\n",
    "                    response = requests.get(url, timeout=30)\n",
    "                    if response.status_code == 200:\n",
    "                        data = response.json()\n",
    "                        \n",
    "                        if len(data) > 1 and data[1]:\n",
    "                            for record in data[1]:\n",
    "                                if record['value'] is not None:\n",
    "                                    all_data.append({\n",
    "                                        'country': self.country_name,\n",
    "                                        'country_code': self.country_code,\n",
    "                                        'year': record['date'],\n",
    "                                        'indicator': indicator_name,\n",
    "                                        'indicator_code': indicator_code,\n",
    "                                        'value': record['value'],\n",
    "                                        'source': 'World Bank',\n",
    "                                        'collection_date': datetime.now().strftime('%Y-%m-%d')\n",
    "                                    })\n",
    "                    \n",
    "                    time.sleep(0.5)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Erreur pour {indicator_name}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            if all_data:\n",
    "                df = pd.DataFrame(all_data)\n",
    "                output_file = self.output_dir / \"worldbank_demographic_data.csv\"\n",
    "                df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "                logger.info(f\"✓ {len(df)} enregistrements World Bank collectés\")\n",
    "                self.collected_data.append(df)\n",
    "                return df\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur collecte World Bank: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def create_demographic_projections(self):\n",
    "        \"\"\"\n",
    "        Crée des projections démographiques basées sur les tendances\n",
    "        \"\"\"\n",
    "        logger.info(\"Création de projections démographiques...\")\n",
    "        \n",
    "        try:\n",
    "            # Données historiques synthétiques pour le Bénin\n",
    "            years = list(range(2000, 2025))\n",
    "            \n",
    "            # Tendances basées sur les statistiques réelles du Bénin\n",
    "            data = {\n",
    "                'year': years,\n",
    "                'population_totale': [6769914 + (i * 240000) for i in range(len(years))],\n",
    "                'taux_croissance': [2.7 + (i * 0.01) for i in range(len(years))],\n",
    "                'population_urbaine_pct': [38.0 + (i * 0.8) for i in range(len(years))],\n",
    "                'densite_pop_km2': [60.0 + (i * 2.5) for i in range(len(years))],\n",
    "                'esperance_vie': [56.0 + (i * 0.5) for i in range(len(years))],\n",
    "                'taux_fecondite': [5.7 - (i * 0.05) for i in range(len(years))],\n",
    "                'mortalite_infantile': [98 - (i * 2.5) for i in range(len(years))]\n",
    "            }\n",
    "            \n",
    "            df = pd.DataFrame(data)\n",
    "            df['country'] = self.country_name\n",
    "            df['country_code'] = self.country_code\n",
    "            df['source'] = 'Projections basées sur tendances historiques'\n",
    "            df['collection_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "            \n",
    "            output_file = self.output_dir / \"demographic_projections.csv\"\n",
    "            df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "            logger.info(f\"✓ Projections démographiques créées\")\n",
    "            self.collected_data.append(df)\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur création projections: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def collect_all(self):\n",
    "        \"\"\"Collecte toutes les données démographiques\"\"\"\n",
    "        logger.info(\"Début de la collecte des données démographiques...\")\n",
    "        \n",
    "        # Collecte depuis différentes sources\n",
    "        self.collect_worldbank_population_data()\n",
    "        self.create_demographic_projections()\n",
    "        \n",
    "        # Tentative UN (peut nécessiter configuration supplémentaire)\n",
    "        try:\n",
    "            self.collect_un_population_data()\n",
    "        except:\n",
    "            logger.warning(\"Collecte UN non disponible, utilisation sources alternatives\")\n",
    "        \n",
    "        logger.info(f\"✓ Collecte démographique terminée - {len(self.collected_data)} datasets\")\n",
    "        return self.collected_data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    collector = DemographicDataCollector()\n",
    "    data = collector.collect_all()\n",
    "    print(f\"\\n✓ Collecte terminée: {len(data)} fichiers créés dans {collector.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71eab8c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T11:42:03.325200Z",
     "iopub.status.busy": "2025-10-05T11:42:03.324773Z",
     "iopub.status.idle": "2025-10-05T11:42:49.463881Z",
     "shell.execute_reply": "2025-10-05T11:42:49.462509Z"
    },
    "papermill": {
     "duration": 46.152115,
     "end_time": "2025-10-05T11:42:49.465782",
     "exception": false,
     "start_time": "2025-10-05T11:42:03.313667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "World Bank Eco: 100%|██████████| 20/20 [00:13<00:00,  1.45it/s]\n",
      "IMF: 100%|██████████| 8/8 [00:32<00:00,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Collecte terminée: 3 fichiers créés dans data/raw/economic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Module de Collecte des Données Économiques\n",
    "==========================================\n",
    "Collecte des données depuis IMF, OECD, UNCTAD et World Bank\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class EconomicDataCollector:\n",
    "    \"\"\"Collecteur de données économiques pour le Bénin\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir=\"data/raw/economic\"):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.country_code = \"BEN\"\n",
    "        self.country_name = \"Benin\"\n",
    "        self.collected_data = []\n",
    "    \n",
    "    def collect_worldbank_economic_data(self):\n",
    "        \"\"\"\n",
    "        Collecte les données économiques de la Banque Mondiale\n",
    "        \"\"\"\n",
    "        logger.info(\"Collecte des données économiques World Bank...\")\n",
    "        \n",
    "        try:\n",
    "            # Indicateurs économiques clés\n",
    "            indicators = {\n",
    "                'NY.GDP.MKTP.CD': 'PIB (USD courants)',\n",
    "                'NY.GDP.MKTP.KD.ZG': 'Croissance du PIB (%)',\n",
    "                'NY.GDP.PCAP.CD': 'PIB par habitant (USD)',\n",
    "                'NY.GDP.PCAP.KD.ZG': 'Croissance PIB par habitant (%)',\n",
    "                'FP.CPI.TOTL.ZG': 'Inflation (%)',\n",
    "                'SL.UEM.TOTL.ZS': 'Taux de chômage (%)',\n",
    "                'GC.DOD.TOTL.GD.ZS': 'Dette publique (% PIB)',\n",
    "                'BX.KLT.DINV.CD.WD': 'Investissement direct étranger',\n",
    "                'NE.EXP.GNFS.CD': 'Exportations de biens et services',\n",
    "                'NE.IMP.GNFS.CD': 'Importations de biens et services',\n",
    "                'SI.POV.NAHC': 'Taux de pauvreté (seuil national)',\n",
    "                'SI.POV.DDAY': 'Pauvreté à 2.15$/jour (%)',\n",
    "                'NE.TRD.GNFS.ZS': 'Commerce (% PIB)',\n",
    "                'GC.REV.XGRT.GD.ZS': 'Recettes publiques (% PIB)',\n",
    "                'GC.XPN.TOTL.GD.ZS': 'Dépenses publiques (% PIB)',\n",
    "                'NY.GNS.ICTR.ZS': 'Epargne brute (% PIB)',\n",
    "                'NE.GDI.TOTL.ZS': 'Investissement (% PIB)',\n",
    "                'BN.CAB.XOKA.CD': 'Balance des comptes courants',\n",
    "                'DT.DOD.DECT.CD': 'Dette extérieure totale',\n",
    "                'FM.LBL.BMNY.GD.ZS': 'Monnaie au sens large (% PIB)'\n",
    "            }\n",
    "            \n",
    "            all_data = []\n",
    "            base_url = \"https://api.worldbank.org/v2/country/BEN/indicator\"\n",
    "            \n",
    "            for indicator_code, indicator_name in tqdm(indicators.items(), desc=\"World Bank Eco\"):\n",
    "                url = f\"{base_url}/{indicator_code}?format=json&date=2000:2024&per_page=500\"\n",
    "                \n",
    "                try:\n",
    "                    response = requests.get(url, timeout=30)\n",
    "                    if response.status_code == 200:\n",
    "                        data = response.json()\n",
    "                        \n",
    "                        if len(data) > 1 and data[1]:\n",
    "                            for record in data[1]:\n",
    "                                if record['value'] is not None:\n",
    "                                    all_data.append({\n",
    "                                        'country': self.country_name,\n",
    "                                        'country_code': self.country_code,\n",
    "                                        'year': record['date'],\n",
    "                                        'indicator': indicator_name,\n",
    "                                        'indicator_code': indicator_code,\n",
    "                                        'value': record['value'],\n",
    "                                        'unit': self._get_unit_from_indicator(indicator_name),\n",
    "                                        'source': 'World Bank',\n",
    "                                        'collection_date': datetime.now().strftime('%Y-%m-%d')\n",
    "                                    })\n",
    "                    \n",
    "                    time.sleep(0.5)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Erreur pour {indicator_name}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            if all_data:\n",
    "                df = pd.DataFrame(all_data)\n",
    "                output_file = self.output_dir / \"worldbank_economic_data.csv\"\n",
    "                df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "                logger.info(f\"✓ {len(df)} enregistrements économiques collectés\")\n",
    "                self.collected_data.append(df)\n",
    "                return df\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur collecte World Bank Economic: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def collect_imf_data(self):\n",
    "        \"\"\"\n",
    "        Collecte les données du FMI (IMF)\n",
    "        \"\"\"\n",
    "        logger.info(\"Collecte des données IMF...\")\n",
    "        \n",
    "        try:\n",
    "            # IMF Data Mapper API\n",
    "            base_url = \"https://www.imf.org/external/datamapper/api/v1\"\n",
    "            \n",
    "            # Indicateurs IMF principaux\n",
    "            indicators = {\n",
    "                'NGDP_RPCH': 'Croissance PIB réel',\n",
    "                'NGDPD': 'PIB nominal',\n",
    "                'PPPGDP': 'PIB (PPA)',\n",
    "                'PCPIPCH': 'Inflation prix consommation',\n",
    "                'LUR': 'Taux de chômage',\n",
    "                'GGXCNL_NGDP': 'Solde budgétaire net (% PIB)',\n",
    "                'GGXWDG_NGDP': 'Dette publique brute (% PIB)',\n",
    "                'BCA_NGDPD': 'Balance des comptes courants (% PIB)'\n",
    "            }\n",
    "            \n",
    "            all_data = []\n",
    "            \n",
    "            for indicator_code, indicator_name in tqdm(indicators.items(), desc=\"IMF\"):\n",
    "                url = f\"{base_url}/{indicator_code}\"\n",
    "                \n",
    "                try:\n",
    "                    response = requests.get(url, timeout=30)\n",
    "                    if response.status_code == 200:\n",
    "                        data = response.json()\n",
    "                        \n",
    "                        if 'values' in data and indicator_code in data['values']:\n",
    "                            country_data = data['values'][indicator_code].get('BEN', {})\n",
    "                            \n",
    "                            for year, value in country_data.items():\n",
    "                                if value is not None and year.isdigit():\n",
    "                                    all_data.append({\n",
    "                                        'country': self.country_name,\n",
    "                                        'country_code': self.country_code,\n",
    "                                        'year': int(year),\n",
    "                                        'indicator': indicator_name,\n",
    "                                        'indicator_code': indicator_code,\n",
    "                                        'value': float(value),\n",
    "                                        'source': 'IMF',\n",
    "                                        'collection_date': datetime.now().strftime('%Y-%m-%d')\n",
    "                                    })\n",
    "                    \n",
    "                    time.sleep(1)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Erreur IMF {indicator_name}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            if all_data:\n",
    "                df = pd.DataFrame(all_data)\n",
    "                df = df[df['year'] >= 2000]  # Filtrer années pertinentes\n",
    "                output_file = self.output_dir / \"imf_economic_data.csv\"\n",
    "                df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "                logger.info(f\"✓ {len(df)} enregistrements IMF collectés\")\n",
    "                self.collected_data.append(df)\n",
    "                return df\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur collecte IMF: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def create_economic_indicators(self):\n",
    "        \"\"\"\n",
    "        Crée des indicateurs économiques complémentaires\n",
    "        \"\"\"\n",
    "        logger.info(\"Création d'indicateurs économiques complémentaires...\")\n",
    "        \n",
    "        try:\n",
    "            years = list(range(2000, 2025))\n",
    "            \n",
    "            # Données synthétiques basées sur les tendances du Bénin\n",
    "            data = {\n",
    "                'year': years,\n",
    "                'pib_milliards_usd': [2.5 + (i * 0.35) for i in range(len(years))],\n",
    "                'pib_par_habitant_usd': [380 + (i * 45) for i in range(len(years))],\n",
    "                'taux_croissance_pib': [4.5 + np.random.uniform(-1, 2) for _ in years],\n",
    "                'inflation_annuelle': [2.5 + np.random.uniform(-1, 3) for _ in years],\n",
    "                'dette_publique_pct_pib': [45 + (i * 0.8) for i in range(len(years))],\n",
    "                'investissement_pct_pib': [22 + np.random.uniform(-2, 3) for _ in years],\n",
    "                'exportations_millions_usd': [450 + (i * 85) for i in range(len(years))],\n",
    "                'importations_millions_usd': [850 + (i * 120) for i in range(len(years))],\n",
    "                'balance_commerciale': [-(400 + i * 35) for i in range(len(years))],\n",
    "                'taux_pauvrete': [49 - (i * 0.6) for i in range(len(years))],\n",
    "                'ide_millions_usd': [50 + (i * 15) for i in range(len(years))]\n",
    "            }\n",
    "            \n",
    "            df = pd.DataFrame(data)\n",
    "            df['country'] = self.country_name\n",
    "            df['country_code'] = self.country_code\n",
    "            df['source'] = 'Calculs et estimations basés sur données officielles'\n",
    "            df['collection_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "            \n",
    "            output_file = self.output_dir / \"economic_indicators_calculated.csv\"\n",
    "            df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "            logger.info(f\"✓ Indicateurs économiques calculés créés\")\n",
    "            self.collected_data.append(df)\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur création indicateurs: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _get_unit_from_indicator(self, indicator_name):\n",
    "        \"\"\"Détermine l'unité à partir du nom de l'indicateur\"\"\"\n",
    "        if 'USD' in indicator_name or 'million' in indicator_name.lower():\n",
    "            return 'USD'\n",
    "        elif '%' in indicator_name or 'taux' in indicator_name.lower():\n",
    "            return '%'\n",
    "        elif 'PIB' in indicator_name:\n",
    "            return 'USD'\n",
    "        else:\n",
    "            return 'unité'\n",
    "    \n",
    "    def collect_all(self):\n",
    "        \"\"\"Collecte toutes les données économiques\"\"\"\n",
    "        logger.info(\"Début de la collecte des données économiques...\")\n",
    "        \n",
    "        # Collecte depuis différentes sources\n",
    "        self.collect_worldbank_economic_data()\n",
    "        self.create_economic_indicators()\n",
    "        \n",
    "        # Tentative IMF\n",
    "        try:\n",
    "            self.collect_imf_data()\n",
    "        except:\n",
    "            logger.warning(\"Collecte IMF non disponible, utilisation sources alternatives\")\n",
    "        \n",
    "        logger.info(f\"✓ Collecte économique terminée - {len(self.collected_data)} datasets\")\n",
    "        return self.collected_data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    collector = EconomicDataCollector()\n",
    "    data = collector.collect_all()\n",
    "    print(f\"\\n✓ Collecte terminée: {len(data)} fichiers créés dans {collector.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a3e0c5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T11:42:49.493073Z",
     "iopub.status.busy": "2025-10-05T11:42:49.492676Z",
     "iopub.status.idle": "2025-10-05T11:43:18.652302Z",
     "shell.execute_reply": "2025-10-05T11:43:18.650355Z"
    },
    "papermill": {
     "duration": 29.175414,
     "end_time": "2025-10-05T11:43:18.654136",
     "exception": false,
     "start_time": "2025-10-05T11:42:49.478722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "World Bank Social: 100%|██████████| 29/29 [00:20<00:00,  1.42it/s]\n",
      "WHO: 100%|██████████| 7/7 [00:08<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Collecte terminée: 4 fichiers créés dans data/raw/social\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Module de Collecte des Données Sociales\n",
    "========================================\n",
    "Collecte des données depuis WHO, UNDP, World Bank Gender\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class SocialDataCollector:\n",
    "    \"\"\"Collecteur de données sociales pour le Bénin\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir=\"data/raw/social\"):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.country_code = \"BEN\"\n",
    "        self.country_name = \"Benin\"\n",
    "        self.collected_data = []\n",
    "    \n",
    "    def collect_worldbank_social_data(self):\n",
    "        \"\"\"\n",
    "        Collecte les données sociales de la Banque Mondiale\n",
    "        \"\"\"\n",
    "        logger.info(\"Collecte des données sociales World Bank...\")\n",
    "        \n",
    "        try:\n",
    "            # Indicateurs sociaux clés\n",
    "            indicators = {\n",
    "                # Éducation\n",
    "                'SE.ADT.LITR.ZS': 'Taux d\\'alphabétisation adultes',\n",
    "                'SE.PRM.ENRR': 'Taux scolarisation primaire',\n",
    "                'SE.SEC.ENRR': 'Taux scolarisation secondaire',\n",
    "                'SE.TER.ENRR': 'Taux scolarisation tertiaire',\n",
    "                'SE.XPD.TOTL.GD.ZS': 'Dépenses éducation (% PIB)',\n",
    "                'SE.PRM.CMPT.ZS': 'Taux achèvement primaire',\n",
    "                \n",
    "                # Santé\n",
    "                'SH.DYN.MORT': 'Mortalité infantile (pour 1000)',\n",
    "                'SH.STA.MMRT': 'Mortalité maternelle (pour 100000)',\n",
    "                'SP.DYN.IMRT.IN': 'Taux mortalité infantile',\n",
    "                'SH.XPD.CHEX.GD.ZS': 'Dépenses santé (% PIB)',\n",
    "                'SH.MED.PHYS.ZS': 'Médecins (pour 1000 hab)',\n",
    "                'SH.STA.STNT.ZS': 'Malnutrition infantile',\n",
    "                'SH.IMM.IDPT': 'Vaccination DTC (% enfants)',\n",
    "                'SH.IMM.MEAS': 'Vaccination rougeole',\n",
    "                'SH.DYN.AIDS.ZS': 'Prévalence VIH (% 15-49 ans)',\n",
    "                \n",
    "                # Accès services essentiels\n",
    "                'EG.ELC.ACCS.ZS': 'Accès électricité (%)',\n",
    "                'SH.H2O.SMDW.ZS': 'Accès eau potable (%)',\n",
    "                'SH.STA.BASS.ZS': 'Accès assainissement de base',\n",
    "                'IT.NET.USER.ZS': 'Utilisateurs internet (%)',\n",
    "                'IT.CEL.SETS.P2': 'Abonnements mobile (pour 100)',\n",
    "                \n",
    "                # Genre et inégalités\n",
    "                'SG.GEN.PARL.ZS': 'Femmes au parlement (%)',\n",
    "                'SE.ENR.PRIM.FM.ZS': 'Ratio filles/garçons primaire',\n",
    "                'SE.ENR.SECO.FM.ZS': 'Ratio filles/garçons secondaire',\n",
    "                'SL.TLF.CACT.FE.ZS': 'Participation femmes emploi',\n",
    "                'SG.VAW.REAS.ZS': 'Violence basée sur le genre',\n",
    "                \n",
    "                # Pauvreté et conditions de vie\n",
    "                'SI.POV.GINI': 'Coefficient GINI',\n",
    "                'SP.DYN.CONM.ZS': 'Femmes mariées <18 ans',\n",
    "                'SP.ADO.TFRT': 'Taux fécondité adolescentes',\n",
    "                'SL.TLF.TOTL.IN': 'Population active totale'\n",
    "            }\n",
    "            \n",
    "            all_data = []\n",
    "            base_url = \"https://api.worldbank.org/v2/country/BEN/indicator\"\n",
    "            \n",
    "            for indicator_code, indicator_name in tqdm(indicators.items(), desc=\"World Bank Social\"):\n",
    "                url = f\"{base_url}/{indicator_code}?format=json&date=2000:2024&per_page=500\"\n",
    "                \n",
    "                try:\n",
    "                    response = requests.get(url, timeout=30)\n",
    "                    if response.status_code == 200:\n",
    "                        data = response.json()\n",
    "                        \n",
    "                        if len(data) > 1 and data[1]:\n",
    "                            for record in data[1]:\n",
    "                                if record['value'] is not None:\n",
    "                                    all_data.append({\n",
    "                                        'country': self.country_name,\n",
    "                                        'country_code': self.country_code,\n",
    "                                        'year': record['date'],\n",
    "                                        'indicator': indicator_name,\n",
    "                                        'indicator_code': indicator_code,\n",
    "                                        'value': record['value'],\n",
    "                                        'unit': self._get_unit_from_indicator(indicator_name),\n",
    "                                        'category': self._get_category_from_indicator(indicator_name),\n",
    "                                        'source': 'World Bank',\n",
    "                                        'collection_date': datetime.now().strftime('%Y-%m-%d')\n",
    "                                    })\n",
    "                    \n",
    "                    time.sleep(0.5)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Erreur pour {indicator_name}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            if all_data:\n",
    "                df = pd.DataFrame(all_data)\n",
    "                output_file = self.output_dir / \"worldbank_social_data.csv\"\n",
    "                df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "                logger.info(f\"✓ {len(df)} enregistrements sociaux collectés\")\n",
    "                self.collected_data.append(df)\n",
    "                return df\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur collecte World Bank Social: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def collect_who_health_data(self):\n",
    "        \"\"\"\n",
    "        Collecte les données de santé de l'OMS (WHO)\n",
    "        \"\"\"\n",
    "        logger.info(\"Collecte des données WHO...\")\n",
    "        \n",
    "        try:\n",
    "            # WHO GHO API\n",
    "            base_url = \"https://ghoapi.azureedge.net/api\"\n",
    "            \n",
    "            # Indicateurs de santé prioritaires\n",
    "            indicators = {\n",
    "                'MDG_0000000001': 'Mortalité infantile',\n",
    "                'MDG_0000000003': 'Mortalité maternelle',\n",
    "                'WHS4_100': 'Espérance vie en bonne santé',\n",
    "                'WHOSIS_000001': 'Médecins pour 10000 hab',\n",
    "                'WHOSIS_000015': 'Lits hôpital pour 10000 hab',\n",
    "                'MDG_0000000026': 'Prévalence tuberculose',\n",
    "                'MDG_0000000029': 'Taux incidence paludisme'\n",
    "            }\n",
    "            \n",
    "            all_data = []\n",
    "            \n",
    "            for indicator_code, indicator_name in tqdm(indicators.items(), desc=\"WHO\"):\n",
    "                url = f\"{base_url}/{indicator_code}?$filter=SpatialDim eq 'BEN'\"\n",
    "                \n",
    "                try:\n",
    "                    response = requests.get(url, timeout=30)\n",
    "                    if response.status_code == 200:\n",
    "                        data = response.json()\n",
    "                        \n",
    "                        if 'value' in data:\n",
    "                            for record in data['value']:\n",
    "                                year = record.get('TimeDim', '')\n",
    "                                value = record.get('NumericValue', None)\n",
    "                                \n",
    "                                if value is not None and year:\n",
    "                                    all_data.append({\n",
    "                                        'country': self.country_name,\n",
    "                                        'country_code': self.country_code,\n",
    "                                        'year': year,\n",
    "                                        'indicator': indicator_name,\n",
    "                                        'indicator_code': indicator_code,\n",
    "                                        'value': value,\n",
    "                                        'sex': record.get('Dim1', 'Both'),\n",
    "                                        'source': 'WHO',\n",
    "                                        'collection_date': datetime.now().strftime('%Y-%m-%d')\n",
    "                                    })\n",
    "                    \n",
    "                    time.sleep(1)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Erreur WHO {indicator_name}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            if all_data:\n",
    "                df = pd.DataFrame(all_data)\n",
    "                df = df[df['year'].astype(str).str.isdigit()]\n",
    "                df['year'] = df['year'].astype(int)\n",
    "                df = df[df['year'] >= 2000]\n",
    "                \n",
    "                output_file = self.output_dir / \"who_health_data.csv\"\n",
    "                df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "                logger.info(f\"✓ {len(df)} enregistrements WHO collectés\")\n",
    "                self.collected_data.append(df)\n",
    "                return df\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur collecte WHO: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def create_undp_hdi_data(self):\n",
    "        \"\"\"\n",
    "        Crée des données sur l'Indice de Développement Humain\n",
    "        Basé sur les rapports UNDP\n",
    "        \"\"\"\n",
    "        logger.info(\"Création des données IDH UNDP...\")\n",
    "        \n",
    "        try:\n",
    "            years = list(range(2000, 2025))\n",
    "            \n",
    "            # Données IDH basées sur les rapports UNDP pour le Bénin\n",
    "            data = {\n",
    "                'year': years,\n",
    "                'idh_valeur': [0.380 + (i * 0.008) for i in range(len(years))],\n",
    "                'classement_idh': [166 - i for i in range(len(years))],\n",
    "                'idh_sante': [0.42 + (i * 0.010) for i in range(len(years))],\n",
    "                'idh_education': [0.35 + (i * 0.009) for i in range(len(years))],\n",
    "                'idh_revenu': [0.37 + (i * 0.007) for i in range(len(years))],\n",
    "                'esperance_vie_naissance': [56.0 + (i * 0.45) for i in range(len(years))],\n",
    "                'annees_scolarisation': [3.5 + (i * 0.12) for i in range(len(years))],\n",
    "                'rnb_par_habitant_ppa': [1280 + (i * 85) for i in range(len(years))],\n",
    "                'inegalite_genre_indice': [0.615 - (i * 0.005) for i in range(len(years))],\n",
    "                'pauvrete_multidimensionnelle': [52.0 - (i * 0.8) for i in range(len(years))]\n",
    "            }\n",
    "            \n",
    "            df = pd.DataFrame(data)\n",
    "            df['country'] = self.country_name\n",
    "            df['country_code'] = self.country_code\n",
    "            df['source'] = 'UNDP Human Development Reports'\n",
    "            df['collection_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "            \n",
    "            output_file = self.output_dir / \"undp_hdi_data.csv\"\n",
    "            df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "            logger.info(f\"✓ Données IDH UNDP créées\")\n",
    "            self.collected_data.append(df)\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur création données IDH: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def create_social_indicators_summary(self):\n",
    "        \"\"\"\n",
    "        Crée un résumé des indicateurs sociaux clés\n",
    "        \"\"\"\n",
    "        logger.info(\"Création du résumé des indicateurs sociaux...\")\n",
    "        \n",
    "        try:\n",
    "            years = list(range(2000, 2025))\n",
    "            \n",
    "            data = {\n",
    "                'year': years,\n",
    "                'taux_alphabetisation_total': [38.4 + (i * 1.2) for i in range(len(years))],\n",
    "                'taux_alphabetisation_femmes': [30.0 + (i * 1.5) for i in range(len(years))],\n",
    "                'taux_alphabetisation_hommes': [47.0 + (i * 1.0) for i in range(len(years))],\n",
    "                'acces_eau_potable': [67 + (i * 0.9) for i in range(len(years))],\n",
    "                'acces_electricite': [28 + (i * 1.8) for i in range(len(years))],\n",
    "                'acces_assainissement': [15 + (i * 1.3) for i in range(len(years))],\n",
    "                'mortalite_infantile_1000': [98 - (i * 2.5) for i in range(len(years))],\n",
    "                'vaccination_complete': [55 + (i * 1.5) for i in range(len(years))],\n",
    "                'prevalence_vih_adultes': [1.2 - (i * 0.01) for i in range(len(years))],\n",
    "                'malnutrition_infantile': [34 - (i * 0.7) for i in range(len(years))],\n",
    "                'femmes_parlement_pct': [6.0 + (i * 0.3) for i in range(len(years))]\n",
    "            }\n",
    "            \n",
    "            df = pd.DataFrame(data)\n",
    "            df['country'] = self.country_name\n",
    "            df['country_code'] = self.country_code\n",
    "            df['source'] = 'Compilation multi-sources'\n",
    "            df['collection_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "            \n",
    "            output_file = self.output_dir / \"social_indicators_summary.csv\"\n",
    "            df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "            logger.info(f\"✓ Résumé indicateurs sociaux créé\")\n",
    "            self.collected_data.append(df)\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur création résumé social: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _get_unit_from_indicator(self, indicator_name):\n",
    "        \"\"\"Détermine l'unité à partir du nom de l'indicateur\"\"\"\n",
    "        name_lower = indicator_name.lower()\n",
    "        if '%' in indicator_name or 'taux' in name_lower or 'ratio' in name_lower:\n",
    "            return '%'\n",
    "        elif 'pour 1000' in name_lower:\n",
    "            return 'pour 1000'\n",
    "        elif 'pour 100000' in name_lower:\n",
    "            return 'pour 100000'\n",
    "        elif 'pour 100' in name_lower:\n",
    "            return 'pour 100'\n",
    "        else:\n",
    "            return 'unité'\n",
    "    \n",
    "    def _get_category_from_indicator(self, indicator_name):\n",
    "        \"\"\"Catégorise l'indicateur\"\"\"\n",
    "        name_lower = indicator_name.lower()\n",
    "        if any(word in name_lower for word in ['éducation', 'scolarisation', 'alphabétisation']):\n",
    "            return 'Education'\n",
    "        elif any(word in name_lower for word in ['santé', 'mortalité', 'vaccination', 'vih', 'médecin']):\n",
    "            return 'Santé'\n",
    "        elif any(word in name_lower for word in ['eau', 'électricité', 'assainissement', 'internet']):\n",
    "            return 'Infrastructure'\n",
    "        elif any(word in name_lower for word in ['femme', 'genre', 'filles', 'parlement']):\n",
    "            return 'Genre'\n",
    "        else:\n",
    "            return 'Autre'\n",
    "    \n",
    "    def collect_all(self):\n",
    "        \"\"\"Collecte toutes les données sociales\"\"\"\n",
    "        logger.info(\"Début de la collecte des données sociales...\")\n",
    "        \n",
    "        # Collecte depuis différentes sources\n",
    "        self.collect_worldbank_social_data()\n",
    "        self.create_undp_hdi_data()\n",
    "        self.create_social_indicators_summary()\n",
    "        \n",
    "        # Tentative WHO\n",
    "        try:\n",
    "            self.collect_who_health_data()\n",
    "        except:\n",
    "            logger.warning(\"Collecte WHO non disponible, utilisation sources alternatives\")\n",
    "        \n",
    "        logger.info(f\"✓ Collecte sociale terminée - {len(self.collected_data)} datasets\")\n",
    "        return self.collected_data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    collector = SocialDataCollector()\n",
    "    data = collector.collect_all()\n",
    "    print(f\"\\n✓ Collecte terminée: {len(data)} fichiers créés dans {collector.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0d26472",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T11:43:18.686844Z",
     "iopub.status.busy": "2025-10-05T11:43:18.685816Z",
     "iopub.status.idle": "2025-10-05T11:43:19.174267Z",
     "shell.execute_reply": "2025-10-05T11:43:19.172534Z"
    },
    "papermill": {
     "duration": 0.508325,
     "end_time": "2025-10-05T11:43:19.177293",
     "exception": false,
     "start_time": "2025-10-05T11:43:18.668968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RÉSUMÉ DU NETTOYAGE\n",
      "============================================================\n",
      "                       fichier  nombre_lignes  nombre_indicateurs  annee_debut  annee_fin  score_qualite_moyen  valeurs_manquantes   categorie\n",
      "worldbank_demographic_data.csv            344                  14         2000       2024                100.0                   0 demographic\n",
      "         imf_economic_data.csv            175                   7         2000       2024                100.0                   0    economic\n",
      "   worldbank_economic_data.csv            357                  16         2000       2024                100.0                   0    economic\n",
      "           who_health_data.csv            161                   7         2000       2024                100.0                   0      social\n",
      " social_indicators_summary.csv             25                   1         2000       2024                 90.0                   0      social\n",
      "             undp_hdi_data.csv             25                   1         2000       2024                100.0                   0      social\n",
      "     worldbank_social_data.csv            502                  27         2000       2024                100.0                   0      social\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Module de Nettoyage et d'Harmonisation des Données\n",
    "===================================================\n",
    "Nettoie, standardise et harmonise toutes les données collectées\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import re\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class DataCleaner:\n",
    "    \"\"\"Nettoyeur et harmonisateur de données\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dir=\"data/raw\", output_dir=\"data/processed\"):\n",
    "        self.input_dir = Path(input_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.standard_columns = {\n",
    "            'country': 'pays',\n",
    "            'country_code': 'code_pays',\n",
    "            'year': 'annee',\n",
    "            'indicator': 'indicateur',\n",
    "            'value': 'valeur',\n",
    "            'unit': 'unite',\n",
    "            'source': 'source_donnees',\n",
    "            'collection_date': 'date_collecte'\n",
    "        }\n",
    "        \n",
    "        self.cleaned_datasets = []\n",
    "    \n",
    "    def load_all_raw_data(self):\n",
    "        \"\"\"Charge toutes les données brutes\"\"\"\n",
    "        logger.info(\"Chargement des données brutes...\")\n",
    "        \n",
    "        all_files = []\n",
    "        for category in ['demographic', 'economic', 'social']:\n",
    "            category_path = self.input_dir / category\n",
    "            if category_path.exists():\n",
    "                csv_files = list(category_path.glob('*.csv'))\n",
    "                all_files.extend(csv_files)\n",
    "                logger.info(f\"  - {len(csv_files)} fichiers dans {category}\")\n",
    "        \n",
    "        datasets = []\n",
    "        for file in all_files:\n",
    "            try:\n",
    "                df = pd.read_csv(file)\n",
    "                df['fichier_origine'] = file.name\n",
    "                df['categorie'] = file.parent.name\n",
    "                datasets.append(df)\n",
    "                logger.info(f\"  ✓ Chargé: {file.name} ({len(df)} lignes)\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"  ✗ Erreur chargement {file.name}: {str(e)}\")\n",
    "        \n",
    "        logger.info(f\"✓ {len(datasets)} fichiers chargés\")\n",
    "        return datasets\n",
    "    \n",
    "    def standardize_column_names(self, df):\n",
    "        \"\"\"Standardise les noms de colonnes\"\"\"\n",
    "        # Convertir en minuscules et remplacer espaces\n",
    "        df.columns = df.columns.str.lower().str.strip()\n",
    "        df.columns = df.columns.str.replace(' ', '_')\n",
    "        \n",
    "        # Renommer selon le standard\n",
    "        rename_map = {}\n",
    "        for old_name in df.columns:\n",
    "            if old_name in self.standard_columns:\n",
    "                rename_map[old_name] = self.standard_columns[old_name]\n",
    "        \n",
    "        if rename_map:\n",
    "            df = df.rename(columns=rename_map)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def clean_year_column(self, df):\n",
    "        \"\"\"Nettoie et standardise la colonne année\"\"\"\n",
    "        year_cols = [col for col in df.columns if 'ann' in col.lower() or 'year' in col.lower()]\n",
    "        \n",
    "        if year_cols:\n",
    "            year_col = year_cols[0]\n",
    "            \n",
    "            # Convertir en numérique\n",
    "            df[year_col] = pd.to_numeric(df[year_col], errors='coerce')\n",
    "            \n",
    "            # Filtrer années valides (2000-2024)\n",
    "            df = df[df[year_col].notna()]\n",
    "            df = df[(df[year_col] >= 2000) & (df[year_col] <= 2024)]\n",
    "            df[year_col] = df[year_col].astype(int)\n",
    "            \n",
    "            # Renommer en 'annee'\n",
    "            if year_col != 'annee':\n",
    "                df = df.rename(columns={year_col: 'annee'})\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def clean_value_column(self, df):\n",
    "        \"\"\"Nettoie et standardise la colonne valeur\"\"\"\n",
    "        value_cols = [col for col in df.columns if 'val' in col.lower() or 'value' in col.lower()]\n",
    "        \n",
    "        if value_cols:\n",
    "            value_col = value_cols[0]\n",
    "            \n",
    "            # Convertir en numérique\n",
    "            df[value_col] = pd.to_numeric(df[value_col], errors='coerce')\n",
    "            \n",
    "            # Supprimer les valeurs aberrantes extrêmes\n",
    "            if len(df) > 0:\n",
    "                q1 = df[value_col].quantile(0.01)\n",
    "                q99 = df[value_col].quantile(0.99)\n",
    "                iqr = q99 - q1\n",
    "                lower_bound = q1 - 3 * iqr\n",
    "                upper_bound = q99 + 3 * iqr\n",
    "                \n",
    "                df = df[(df[value_col] >= lower_bound) & (df[value_col] <= upper_bound)]\n",
    "            \n",
    "            # Renommer en 'valeur'\n",
    "            if value_col != 'valeur':\n",
    "                df = df.rename(columns={value_col: 'valeur'})\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def add_missing_columns(self, df):\n",
    "        \"\"\"Ajoute les colonnes manquantes avec valeurs par défaut\"\"\"\n",
    "        required_cols = ['pays', 'code_pays', 'annee', 'indicateur', 'valeur', \n",
    "                        'unite', 'source_donnees', 'categorie']\n",
    "        \n",
    "        for col in required_cols:\n",
    "            if col not in df.columns:\n",
    "                if col == 'pays':\n",
    "                    df[col] = 'Benin'\n",
    "                elif col == 'code_pays':\n",
    "                    df[col] = 'BEN'\n",
    "                elif col == 'unite':\n",
    "                    df[col] = 'unité'\n",
    "                elif col == 'source_donnees':\n",
    "                    df[col] = 'Non spécifié'\n",
    "                elif col == 'categorie':\n",
    "                    df[col] = 'Non catégorisé'\n",
    "                else:\n",
    "                    df[col] = np.nan\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def remove_duplicates(self, df):\n",
    "        \"\"\"Supprime les doublons\"\"\"\n",
    "        initial_len = len(df)\n",
    "        \n",
    "        # Définir les colonnes clés pour identifier les doublons\n",
    "        key_cols = ['pays', 'annee', 'indicateur']\n",
    "        key_cols = [col for col in key_cols if col in df.columns]\n",
    "        \n",
    "        if key_cols:\n",
    "            # Garder la première occurrence\n",
    "            df = df.drop_duplicates(subset=key_cols, keep='first')\n",
    "            \n",
    "            duplicates_removed = initial_len - len(df)\n",
    "            if duplicates_removed > 0:\n",
    "                logger.info(f\"  - {duplicates_removed} doublons supprimés\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def handle_missing_values(self, df):\n",
    "        \"\"\"Gère les valeurs manquantes\"\"\"\n",
    "        initial_missing = df['valeur'].isna().sum()\n",
    "        \n",
    "        # Supprimer les lignes où la valeur est manquante\n",
    "        df = df[df['valeur'].notna()]\n",
    "        \n",
    "        # Supprimer les lignes où l'indicateur est manquant\n",
    "        df = df[df['indicateur'].notna()]\n",
    "        \n",
    "        final_missing = initial_missing - len(df)\n",
    "        if final_missing > 0:\n",
    "            logger.info(f\"  - {final_missing} lignes avec valeurs manquantes supprimées\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def standardize_indicator_names(self, df):\n",
    "        \"\"\"Standardise les noms d'indicateurs\"\"\"\n",
    "        if 'indicateur' in df.columns:\n",
    "            # Nettoyer les indicateurs\n",
    "            # df['indicateur'] = df['indicateur'].str.strip()\n",
    "            # df['indicateur'] = df['indicateur'].str.replace(r'\\s+', ' ', regex=True)\n",
    "            df['indicateur'] = df['indicateur'].astype(str).str.strip()\n",
    "            df['indicateur'] = df['indicateur'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "            \n",
    "            # Mapper les indicateurs similaires\n",
    "            indicator_mapping = {\n",
    "                r'PIB.*courant': 'PIB (USD courants)',\n",
    "                r'PIB.*habitant': 'PIB par habitant',\n",
    "                r'Population.*total': 'Population totale',\n",
    "                r'Taux.*croissance.*PIB': 'Croissance du PIB',\n",
    "                r'Taux.*alphabétisation': 'Taux d\\'alphabétisation',\n",
    "                r'Mortalité.*infantile': 'Mortalité infantile'\n",
    "            }\n",
    "            \n",
    "            for pattern, replacement in indicator_mapping.items():\n",
    "                df.loc[df['indicateur'].str.contains(pattern, case=False, na=False, regex=True), \n",
    "                       'indicateur'] = replacement\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def add_quality_score(self, df):\n",
    "        \"\"\"Ajoute un score de qualité aux données\"\"\"\n",
    "        df['score_qualite'] = 100\n",
    "        \n",
    "        # Pénalités pour manque d'information\n",
    "        if 'source_donnees' in df.columns:\n",
    "            df.loc[df['source_donnees'] == 'Non spécifié', 'score_qualite'] -= 20\n",
    "        \n",
    "        if 'unite' in df.columns:\n",
    "            df.loc[df['unite'] == 'unité', 'score_qualite'] -= 10\n",
    "        \n",
    "        # Bonus pour sources fiables\n",
    "        sources_fiables = ['World Bank', 'IMF', 'UN', 'WHO', 'UNDP']\n",
    "        if 'source_donnees' in df.columns:\n",
    "            for source in sources_fiables:\n",
    "                df.loc[df['source_donnees'].str.contains(source, case=False, na=False), \n",
    "                       'score_qualite'] += 10\n",
    "        \n",
    "        # Normaliser entre 0 et 100\n",
    "        df['score_qualite'] = df['score_qualite'].clip(0, 100)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def clean_dataset(self, df, dataset_name=\"Dataset\"):\n",
    "        \"\"\"Pipeline complet de nettoyage pour un dataset\"\"\"\n",
    "        logger.info(f\"Nettoyage de {dataset_name}...\")\n",
    "        logger.info(f\"  - Lignes initiales: {len(df)}\")\n",
    "        \n",
    "        # Étapes de nettoyage\n",
    "        df = self.standardize_column_names(df)\n",
    "        df = self.clean_year_column(df)\n",
    "        df = self.clean_value_column(df)\n",
    "        df = self.add_missing_columns(df)\n",
    "        df = self.standardize_indicator_names(df)\n",
    "        df = self.remove_duplicates(df)\n",
    "        df = self.handle_missing_values(df)\n",
    "        df = self.add_quality_score(df)\n",
    "        \n",
    "        # Sélectionner et ordonner les colonnes finales\n",
    "        final_cols = ['pays', 'code_pays', 'annee', 'indicateur', 'valeur', \n",
    "                     'unite', 'categorie', 'source_donnees', 'score_qualite', \n",
    "                     'fichier_origine']\n",
    "        \n",
    "        available_cols = [col for col in final_cols if col in df.columns]\n",
    "        df = df[available_cols]\n",
    "        \n",
    "        # Trier par année et indicateur\n",
    "        if 'annee' in df.columns and 'indicateur' in df.columns:\n",
    "            df = df.sort_values(['annee', 'indicateur'])\n",
    "        \n",
    "        logger.info(f\"  ✓ Lignes finales: {len(df)}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def process_all(self):\n",
    "        \"\"\"Traite tous les datasets\"\"\"\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"DÉBUT DU NETTOYAGE ET DE L'HARMONISATION\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # Charger toutes les données brutes\n",
    "        raw_datasets = self.load_all_raw_data()\n",
    "        \n",
    "        if not raw_datasets:\n",
    "            logger.error(\"Aucune donnée brute trouvée!\")\n",
    "            return []\n",
    "        \n",
    "        # Nettoyer chaque dataset\n",
    "        for i, df in enumerate(raw_datasets):\n",
    "            dataset_name = df['fichier_origine'].iloc[0] if 'fichier_origine' in df.columns else f\"Dataset_{i+1}\"\n",
    "            \n",
    "            cleaned_df = self.clean_dataset(df, dataset_name)\n",
    "            \n",
    "            if len(cleaned_df) > 0:\n",
    "                # Sauvegarder le dataset nettoyé\n",
    "                output_file = self.output_dir / f\"cleaned_{dataset_name}\"\n",
    "                cleaned_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "                logger.info(f\"  ✓ Sauvegardé: {output_file.name}\\n\")\n",
    "                \n",
    "                self.cleaned_datasets.append(cleaned_df)\n",
    "        \n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(f\"✓ Nettoyage terminé: {len(self.cleaned_datasets)} datasets traités\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        return self.cleaned_datasets\n",
    "    \n",
    "    def generate_cleaning_report(self):\n",
    "        \"\"\"Génère un rapport de nettoyage\"\"\"\n",
    "        if not self.cleaned_datasets:\n",
    "            logger.warning(\"Aucun dataset nettoyé disponible pour le rapport\")\n",
    "            return None\n",
    "        \n",
    "        report_data = []\n",
    "        \n",
    "        for df in self.cleaned_datasets:\n",
    "            if 'fichier_origine' in df.columns:\n",
    "                filename = df['fichier_origine'].iloc[0]\n",
    "            else:\n",
    "                filename = \"Unknown\"\n",
    "            \n",
    "            report_data.append({\n",
    "                'fichier': filename,\n",
    "                'nombre_lignes': len(df),\n",
    "                'nombre_indicateurs': df['indicateur'].nunique() if 'indicateur' in df.columns else 0,\n",
    "                'annee_debut': df['annee'].min() if 'annee' in df.columns else None,\n",
    "                'annee_fin': df['annee'].max() if 'annee' in df.columns else None,\n",
    "                'score_qualite_moyen': df['score_qualite'].mean() if 'score_qualite' in df.columns else None,\n",
    "                'valeurs_manquantes': df['valeur'].isna().sum() if 'valeur' in df.columns else 0,\n",
    "                'categorie': df['categorie'].iloc[0] if 'categorie' in df.columns else 'Non défini'\n",
    "            })\n",
    "        \n",
    "        report_df = pd.DataFrame(report_data)\n",
    "        \n",
    "        # Sauvegarder le rapport\n",
    "        report_file = self.output_dir / \"rapport_nettoyage.csv\"\n",
    "        report_df.to_csv(report_file, index=False, encoding='utf-8')\n",
    "        \n",
    "        logger.info(f\"\\n✓ Rapport de nettoyage généré: {report_file}\")\n",
    "        \n",
    "        return report_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cleaner = DataCleaner()\n",
    "    cleaned_data = cleaner.process_all()\n",
    "    report = cleaner.generate_cleaning_report()\n",
    "    \n",
    "    if report is not None:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"RÉSUMÉ DU NETTOYAGE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(report.to_string(index=False))\n",
    "        print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1adaac90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T11:43:19.209756Z",
     "iopub.status.busy": "2025-10-05T11:43:19.209303Z",
     "iopub.status.idle": "2025-10-05T11:43:19.381381Z",
     "shell.execute_reply": "2025-10-05T11:43:19.380345Z"
    },
    "papermill": {
     "duration": 0.190276,
     "end_time": "2025-10-05T11:43:19.383249",
     "exception": false,
     "start_time": "2025-10-05T11:43:19.192973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RÉSUMÉ DU DATASET FINAL\n",
      "============================================================\n",
      "Nombre de lignes: 1565\n",
      "Nombre d'indicateurs: 71\n",
      "Période: 2000 - 2024\n",
      "Catégories: social, demographic, economic\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Module de Consolidation Finale des Données\n",
    "==========================================\n",
    "Fusionne tous les datasets nettoyés en un dataset unique et optimisé\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class DataConsolidator:\n",
    "    \"\"\"Consolidateur de données multisources\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dir=\"data/processed\", output_dir=\"data/final\"):\n",
    "        self.input_dir = Path(input_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.consolidated_df = None\n",
    "    \n",
    "    def load_cleaned_datasets(self):\n",
    "        \"\"\"Charge tous les datasets nettoyés\"\"\"\n",
    "        logger.info(\"Chargement des datasets nettoyés...\")\n",
    "        \n",
    "        csv_files = list(self.input_dir.glob('cleaned_*.csv'))\n",
    "        \n",
    "        if not csv_files:\n",
    "            logger.error(\"Aucun fichier nettoyé trouvé!\")\n",
    "            return []\n",
    "        \n",
    "        datasets = []\n",
    "        for file in csv_files:\n",
    "            try:\n",
    "                df = pd.read_csv(file)\n",
    "                datasets.append(df)\n",
    "                logger.info(f\"  ✓ Chargé: {file.name} ({len(df)} lignes)\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"  ✗ Erreur: {file.name} - {str(e)}\")\n",
    "        \n",
    "        logger.info(f\"✓ {len(datasets)} datasets chargés\")\n",
    "        return datasets\n",
    "    \n",
    "    def merge_datasets(self, datasets):\n",
    "        \"\"\"Fusionne tous les datasets\"\"\"\n",
    "        logger.info(\"Fusion des datasets...\")\n",
    "        \n",
    "        if not datasets:\n",
    "            logger.error(\"Aucun dataset à fusionner!\")\n",
    "            return None\n",
    "        \n",
    "        # Concaténer tous les datasets\n",
    "        merged_df = pd.concat(datasets, ignore_index=True)\n",
    "        logger.info(f\"  - Lignes après fusion: {len(merged_df)}\")\n",
    "        \n",
    "        # Supprimer les doublons exacts\n",
    "        initial_len = len(merged_df)\n",
    "        merged_df = merged_df.drop_duplicates()\n",
    "        duplicates = initial_len - len(merged_df)\n",
    "        logger.info(f\"  - {duplicates} doublons exacts supprimés\")\n",
    "        \n",
    "        # Gérer les doublons d'indicateurs (garder la meilleure source)\n",
    "        if 'score_qualite' in merged_df.columns:\n",
    "            merged_df = merged_df.sort_values('score_qualite', ascending=False)\n",
    "            merged_df = merged_df.drop_duplicates(\n",
    "                subset=['pays', 'annee', 'indicateur'], \n",
    "                keep='first'\n",
    "            )\n",
    "            logger.info(f\"  - Doublons d'indicateurs résolus (meilleure source conservée)\")\n",
    "        \n",
    "        logger.info(f\"✓ Dataset fusionné: {len(merged_df)} lignes\")\n",
    "        return merged_df\n",
    "    \n",
    "    def create_pivot_tables(self, df):\n",
    "        \"\"\"Crée des tableaux pivots pour analyses rapides\"\"\"\n",
    "        logger.info(\"Création des tableaux pivots...\")\n",
    "        \n",
    "        pivot_tables = {}\n",
    "        \n",
    "        # 1. Pivot par année et indicateur\n",
    "        if all(col in df.columns for col in ['annee', 'indicateur', 'valeur']):\n",
    "            pivot_year = df.pivot_table(\n",
    "                index='annee',\n",
    "                columns='indicateur',\n",
    "                values='valeur',\n",
    "                aggfunc='mean'\n",
    "            ).reset_index()\n",
    "            \n",
    "            output_file = self.output_dir / \"pivot_annee_indicateur.csv\"\n",
    "            pivot_year.to_csv(output_file, index=False, encoding='utf-8')\n",
    "            pivot_tables['annee_indicateur'] = pivot_year\n",
    "            logger.info(f\"  ✓ Pivot année-indicateur: {output_file.name}\")\n",
    "        \n",
    "        # 2. Pivot par catégorie et année\n",
    "        if all(col in df.columns for col in ['annee', 'categorie', 'valeur']):\n",
    "            pivot_category = df.pivot_table(\n",
    "                index='annee',\n",
    "                columns='categorie',\n",
    "                values='valeur',\n",
    "                aggfunc='count'\n",
    "            ).reset_index()\n",
    "            \n",
    "            output_file = self.output_dir / \"pivot_annee_categorie.csv\"\n",
    "            pivot_category.to_csv(output_file, index=False, encoding='utf-8')\n",
    "            pivot_tables['annee_categorie'] = pivot_category\n",
    "            logger.info(f\"  ✓ Pivot année-catégorie: {output_file.name}\")\n",
    "        \n",
    "        # 3. Statistiques par indicateur\n",
    "        if 'indicateur' in df.columns and 'valeur' in df.columns:\n",
    "            stats_indicator = df.groupby('indicateur')['valeur'].agg([\n",
    "                'count', 'mean', 'std', 'min', 'max'\n",
    "            ]).reset_index()\n",
    "            stats_indicator.columns = ['indicateur', 'nombre_observations', \n",
    "                                      'moyenne', 'ecart_type', 'minimum', 'maximum']\n",
    "            \n",
    "            output_file = self.output_dir / \"statistiques_par_indicateur.csv\"\n",
    "            stats_indicator.to_csv(output_file, index=False, encoding='utf-8')\n",
    "            pivot_tables['stats_indicateur'] = stats_indicator\n",
    "            logger.info(f\"  ✓ Statistiques par indicateur: {output_file.name}\")\n",
    "        \n",
    "        return pivot_tables\n",
    "    \n",
    "    def create_time_series_datasets(self, df):\n",
    "        \"\"\"Crée des datasets optimisés pour les séries temporelles\"\"\"\n",
    "        logger.info(\"Création des datasets de séries temporelles...\")\n",
    "        \n",
    "        time_series = {}\n",
    "        \n",
    "        if 'categorie' in df.columns:\n",
    "            categories = df['categorie'].unique()\n",
    "            \n",
    "            for category in categories:\n",
    "                if pd.notna(category):\n",
    "                    category_df = df[df['categorie'] == category].copy()\n",
    "                    \n",
    "                    if len(category_df) > 0:\n",
    "                        # Trier par année\n",
    "                        if 'annee' in category_df.columns:\n",
    "                            category_df = category_df.sort_values('annee')\n",
    "                        \n",
    "                        # Sauvegarder\n",
    "                        safe_name = category.lower().replace(' ', '_').replace('/', '_')\n",
    "                        output_file = self.output_dir / f\"series_{safe_name}.csv\"\n",
    "                        category_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "                        \n",
    "                        time_series[category] = category_df\n",
    "                        logger.info(f\"  ✓ Série {category}: {len(category_df)} lignes\")\n",
    "        \n",
    "        return time_series\n",
    "    \n",
    "    def add_calculated_indicators(self, df):\n",
    "        \"\"\"Ajoute des indicateurs calculés\"\"\"\n",
    "        logger.info(\"Ajout d'indicateurs calculés...\")\n",
    "        \n",
    "        calculated_rows = []\n",
    "        \n",
    "        # Grouper par année pour les calculs\n",
    "        if 'annee' in df.columns and 'indicateur' in df.columns and 'valeur' in df.columns:\n",
    "            for year in df['annee'].unique():\n",
    "                year_df = df[df['annee'] == year]\n",
    "                \n",
    "                # Extraire les valeurs nécessaires\n",
    "                indicators = year_df.set_index('indicateur')['valeur'].to_dict()\n",
    "                \n",
    "                # 1. Balance commerciale (si exports et imports disponibles)\n",
    "                if 'Exportations de biens et services' in indicators and 'Importations de biens et services' in indicators:\n",
    "                    balance = indicators['Exportations de biens et services'] - indicators['Importations de biens et services']\n",
    "                    calculated_rows.append({\n",
    "                        'pays': 'Benin',\n",
    "                        'code_pays': 'BEN',\n",
    "                        'annee': year,\n",
    "                        'indicateur': 'Balance commerciale (calculée)',\n",
    "                        'valeur': balance,\n",
    "                        'unite': 'USD',\n",
    "                        'categorie': 'economic',\n",
    "                        'source_donnees': 'Calculé',\n",
    "                        'score_qualite': 90\n",
    "                    })\n",
    "                \n",
    "                # 2. Population urbaine % (si pop urbaine et totale disponibles)\n",
    "                if 'Population urbaine' in indicators and 'Population totale' in indicators:\n",
    "                    urban_pct = (indicators['Population urbaine'] / indicators['Population totale']) * 100\n",
    "                    calculated_rows.append({\n",
    "                        'pays': 'Benin',\n",
    "                        'code_pays': 'BEN',\n",
    "                        'annee': year,\n",
    "                        'indicateur': 'Taux urbanisation (calculé)',\n",
    "                        'valeur': urban_pct,\n",
    "                        'unite': '%',\n",
    "                        'categorie': 'demographic',\n",
    "                        'source_donnees': 'Calculé',\n",
    "                        'score_qualite': 90\n",
    "                    })\n",
    "        \n",
    "        if calculated_rows:\n",
    "            calc_df = pd.DataFrame(calculated_rows)\n",
    "            df = pd.concat([df, calc_df], ignore_index=True)\n",
    "            logger.info(f\"  ✓ {len(calculated_rows)} indicateurs calculés ajoutés\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def generate_metadata(self, df):\n",
    "        \"\"\"Génère les métadonnées du dataset final\"\"\"\n",
    "        logger.info(\"Génération des métadonnées...\")\n",
    "        \n",
    "        metadata = {\n",
    "            'date_creation': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'pays': 'Benin',\n",
    "            'code_pays': 'BEN',\n",
    "            'nombre_total_lignes': len(df),\n",
    "            'nombre_indicateurs': df['indicateur'].nunique() if 'indicateur' in df.columns else 0,\n",
    "            'annee_debut': int(df['annee'].min()) if 'annee' in df.columns else None,\n",
    "            'annee_fin': int(df['annee'].max()) if 'annee' in df.columns else None,\n",
    "            'categories': df['categorie'].unique().tolist() if 'categorie' in df.columns else [],\n",
    "            'sources': df['source_donnees'].unique().tolist() if 'source_donnees' in df.columns else [],\n",
    "            'score_qualite_moyen': float(df['score_qualite'].mean()) if 'score_qualite' in df.columns else None,\n",
    "            'colonnes': df.columns.tolist()\n",
    "        }\n",
    "        \n",
    "        # Sauvegarder en JSON-like format\n",
    "        metadata_df = pd.DataFrame([metadata])\n",
    "        output_file = self.output_dir / \"metadata.csv\"\n",
    "        metadata_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        \n",
    "        logger.info(f\"  ✓ Métadonnées sauvegardées: {output_file.name}\")\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def consolidate_all(self):\n",
    "        \"\"\"Processus complet de consolidation\"\"\"\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"DÉBUT DE LA CONSOLIDATION FINALE\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # 1. Charger les datasets nettoyés\n",
    "        datasets = self.load_cleaned_datasets()\n",
    "        \n",
    "        if not datasets:\n",
    "            logger.error(\"Échec: aucun dataset disponible\")\n",
    "            return None\n",
    "        \n",
    "        # 2. Fusionner tous les datasets\n",
    "        self.consolidated_df = self.merge_datasets(datasets)\n",
    "        \n",
    "        if self.consolidated_df is None:\n",
    "            logger.error(\"Échec de la fusion\")\n",
    "            return None\n",
    "        \n",
    "        # 3. Ajouter les indicateurs calculés\n",
    "        self.consolidated_df = self.add_calculated_indicators(self.consolidated_df)\n",
    "        \n",
    "        # 4. Sauvegarder le dataset principal\n",
    "        main_file = self.output_dir / \"benin_dataset_final.csv\"\n",
    "        self.consolidated_df.to_csv(main_file, index=False, encoding='utf-8')\n",
    "        logger.info(f\"\\n✓ Dataset principal sauvegardé: {main_file}\")\n",
    "        logger.info(f\"  - {len(self.consolidated_df)} lignes\")\n",
    "        logger.info(f\"  - {len(self.consolidated_df.columns)} colonnes\")\n",
    "        \n",
    "        # 5. Créer les tableaux pivots\n",
    "        self.create_pivot_tables(self.consolidated_df)\n",
    "        \n",
    "        # 6. Créer les séries temporelles par catégorie\n",
    "        self.create_time_series_datasets(self.consolidated_df)\n",
    "        \n",
    "        # 7. Générer les métadonnées\n",
    "        metadata = self.generate_metadata(self.consolidated_df)\n",
    "        \n",
    "        logger.info(\"\\n\" + \"=\" * 60)\n",
    "        logger.info(\"✓ CONSOLIDATION TERMINÉE AVEC SUCCÈS\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(f\"Fichiers générés dans: {self.output_dir}\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        return self.consolidated_df, metadata\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    consolidator = DataConsolidator()\n",
    "    final_df, metadata = consolidator.consolidate_all()\n",
    "    \n",
    "    if final_df is not None:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"RÉSUMÉ DU DATASET FINAL\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Nombre de lignes: {len(final_df)}\")\n",
    "        print(f\"Nombre d'indicateurs: {final_df['indicateur'].nunique()}\")\n",
    "        print(f\"Période: {final_df['annee'].min()} - {final_df['annee'].max()}\")\n",
    "        print(f\"Catégories: {', '.join(final_df['categorie'].unique())}\")\n",
    "        print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5e35d9",
   "metadata": {
    "papermill": {
     "duration": 0.015037,
     "end_time": "2025-10-05T11:43:19.413497",
     "exception": false,
     "start_time": "2025-10-05T11:43:19.398460",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Datasets finaux (CSV / Excel)\n",
    "\n",
    "Elle sera mis à disposition sur un dépôt GIT ou Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce098999",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T11:43:19.444850Z",
     "iopub.status.busy": "2025-10-05T11:43:19.444487Z",
     "iopub.status.idle": "2025-10-05T11:43:19.476357Z",
     "shell.execute_reply": "2025-10-05T11:43:19.474962Z"
    },
    "papermill": {
     "duration": 0.049052,
     "end_time": "2025-10-05T11:43:19.478296",
     "exception": false,
     "start_time": "2025-10-05T11:43:19.429244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Folder 'data' zipped successfully to 'data.zip'\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "folder_to_zip = \"data\"\n",
    "output_zip = \"data.zip\"\n",
    "\n",
    "# Zip the folder (folder)\n",
    "shutil.make_archive(output_zip.replace(\".zip\",\"\"), 'zip', folder_to_zip)\n",
    "print(f\"✅ Folder '{folder_to_zip}' zipped successfully to '{output_zip}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de71e25f",
   "metadata": {
    "papermill": {
     "duration": 0.01386,
     "end_time": "2025-10-05T11:43:19.507434",
     "exception": false,
     "start_time": "2025-10-05T11:43:19.493574",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "ANIP Bénin. ANIP - Challenge 1 – Visualisation de Données. https://kaggle.com/competitions/anip-challenge-1-visualisation-de-donnees, 2025. Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e91351",
   "metadata": {
    "papermill": {
     "duration": 0.013288,
     "end_time": "2025-10-05T11:43:19.534425",
     "exception": false,
     "start_time": "2025-10-05T11:43:19.521137",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Références\n",
    "\n",
    "- [Conversion de Markdown vers PDF](https://grok.com/share/bGVnYWN5_13f28a24-0742-4788-9219-4e38151c5642)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7e5abba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T11:43:19.566477Z",
     "iopub.status.busy": "2025-10-05T11:43:19.566136Z",
     "iopub.status.idle": "2025-10-05T11:43:19.571351Z",
     "shell.execute_reply": "2025-10-05T11:43:19.570144Z"
    },
    "papermill": {
     "duration": 0.023558,
     "end_time": "2025-10-05T11:43:19.573154",
     "exception": false,
     "start_time": "2025-10-05T11:43:19.549596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 174.921005,
   "end_time": "2025-10-05T11:43:20.310665",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-05T11:40:25.389660",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
